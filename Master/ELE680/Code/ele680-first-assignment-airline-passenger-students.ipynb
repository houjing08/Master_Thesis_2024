{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"47622125-f9a5-477a-91ee-4b4e5f342bca","_uuid":"2e2cea7e-2153-41ae-bda7-b3ac3971e967","trusted":true},"source":["## AIRLANE PASSENGER SATISFACTION PREDICTION - ASSIGNMENT 1"]},{"cell_type":"markdown","metadata":{"_cell_guid":"87f97167-c070-433d-b36a-dcf42002de85","_uuid":"3e63f370-034f-4826-a26f-5c0ecc5dcc73","execution":{"iopub.execute_input":"2023-07-06T09:03:36.293316Z","iopub.status.busy":"2023-07-06T09:03:36.292855Z","iopub.status.idle":"2023-07-06T09:03:36.306847Z","shell.execute_reply":"2023-07-06T09:03:36.305601Z","shell.execute_reply.started":"2023-07-06T09:03:36.293282Z"},"trusted":true},"source":["In this first assignment, we will work with tabular data in order to deploy a simple Multilayer Perceptron (MLP). In particular, we will *predict the level of satisfaction of airline passengers*. The tasks to be implemented are split in three main sections:\n","\n","* Exploratoy Data Analysis (EDA)\n","* Data Preprocessing\n","* MLP Implementation\n","\n","In the first part of the assignment we will perform what is commonly known as an exploratory data analysis (EDA), in which we aim to gain some knowledge about the dataset and get to understand the nature of the data we're dealing with. We will get familiar with **Pandas**, a really useful **Python** package when dealing with tabular data. Such exploratory analysis will allow us to know more about the distribution of our data and find anomalies that can potencially affect the training of our AI model.\n","\n","Then, we will develop a feature engineering pipeline to preprocess our data. Here, we can decide how we want to deal with the anomalies previously found and modify/transform/prepare our data before being used for training (for instance, it is quite common to include some feature standardization). At the same time, this step usually includes the data splitting into train, validation and test.\n","\n","Finally, the MLP deployment will be carried out with **Tensorflow**, one of the most commonly used Python packages when it comes deep learning (DL) architectures. We will code from scratch our model and we will see how to implement the basic steps to train it and test it using a proper data splitting. This is essential in order to have a \"fair\" evaluation of the methodology used in our project.\n","\n","But enough of theoretical explanations, let's get dirty!\n","\n","**NOTE**: Througout the different tasks in this assignment, you will find some questions marked as **Q**. These questions should be answered at the end of the Notebook (there is a Markdown cell prepared for this purpose)."]},{"cell_type":"markdown","metadata":{"_cell_guid":"eda0dfd9-ceec-467b-990d-6fb6ce0a6e5c","_uuid":"e741e429-9912-4e79-8cbf-374170c1635d","trusted":true},"source":["### Exploratory Data Analysis (EDA)\n","\n","The first thing you'll need to do, is to load the dataset from Kaggle. To do that, we need to add the dataset to our notebook. This can be done by clicking in *Add data* in the top-right corner of the notebook and searching for **Airline Passenger Satisfaction**. Once the dataset has been added (it should appear in our notebook's input, again in the top-right corner), we can start. If you want to learn more about the information contained in this dataset, check https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"612e2b3b-14de-42c0-9800-3e8e6b6cfb03","_uuid":"e6446739-7c07-419f-88c9-f50e779e395c","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:00.426225Z","iopub.status.busy":"2023-07-06T13:54:00.425705Z","iopub.status.idle":"2023-07-06T13:54:04.877884Z","shell.execute_reply":"2023-07-06T13:54:04.876588Z","shell.execute_reply.started":"2023-07-06T13:54:00.426190Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Importing the right packages in our Python 3 environment. In Kaggle, most of the well-known packages are already installed.\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras import callbacks\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras import Sequential\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","import seaborn as sns\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","metadata":{"_cell_guid":"1ef483b3-ed81-401b-b9e0-94b1b9f392f8","_uuid":"e7125486-b6cf-4088-9ce7-b6d73aa9b5a3","trusted":true},"source":[" **Task 1** \n"," \n","Read the training data using read_csv and the right path. \"Display\" (with head()) the dataframe to get an overview of columns (features) that are included in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a059018-edc4-4cac-a529-bf73b4e3f6a9","_uuid":"7da0b69c-cf47-42bb-91a2-bb78f85d5446","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:04.881263Z","iopub.status.busy":"2023-07-06T13:54:04.880540Z","iopub.status.idle":"2023-07-06T13:54:05.394431Z","shell.execute_reply":"2023-07-06T13:54:05.392917Z","shell.execute_reply.started":"2023-07-06T13:54:04.881226Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: Input data files are available in the read-only \"../input/\" directory\n","\n","train_df = pd.read_csv(, index_col=0) # Type your solution\n","train_df. # Type your solution"]},{"cell_type":"markdown","metadata":{"_cell_guid":"3a509db5-78a3-4dc7-abeb-1c146482bbc8","_uuid":"0b94e8c3-e9e9-4b4d-aca2-77b2b55088f6","trusted":true},"source":["**Task 2** \n","\n","It is always important to get familiar with our data in order to make the proper decisions. To begin with it, let's see the type of data we are working with (use info())."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fe9ac09-de0a-40d7-bffe-131b34dd677c","_uuid":"2bd6957f-d4f9-42e2-98fb-6a489eb55c06","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:05.396405Z","iopub.status.busy":"2023-07-06T13:54:05.396001Z","iopub.status.idle":"2023-07-06T13:54:05.581456Z","shell.execute_reply":"2023-07-06T13:54:05.580496Z","shell.execute_reply.started":"2023-07-06T13:54:05.396369Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["train_df. # Type your solution"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f06b38d0-566d-4655-bc69-5b49a3381680","_uuid":"ca36de70-de47-4e02-a332-827fe744c1aa","trusted":true},"source":["**Task 3** \n","\n","We can also focus on some of the features to get an initial idea of the distribution of our dataset. For instance, let's check how many instances we have for the column \"satisfaction\"."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"166eb4fe-c69b-471e-a541-b8e25414228a","_uuid":"1e1c1fd2-5db0-4f95-aab1-02f886e37dd8","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:05.584397Z","iopub.status.busy":"2023-07-06T13:54:05.583661Z","iopub.status.idle":"2023-07-06T13:54:05.724561Z","shell.execute_reply":"2023-07-06T13:54:05.722784Z","shell.execute_reply.started":"2023-07-06T13:54:05.584363Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: use np.unique with return counts. \n","\n","unique, count = np.unique(, ) # type your solution\n","print(\"The number of occurances of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )"]},{"cell_type":"markdown","metadata":{"_cell_guid":"837d0b75-9b5c-49cc-adc0-19ceb7adc511","_uuid":"da01f076-5336-44a2-97f8-5cb32e2b1b72","trusted":true},"source":["**Task 4** \n","\n","As mentioned before, anomalies are an important aspect when dealing with tabular data. Missing values is one of the most common and one of the first choices we might need to make is how to deal with them. There are different ways of doing so: mean/median imputation, predict the missing value using other features, drop those samples... Check the number of missings for every feature in our data!"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dda4701d-b962-4d2f-96cf-ec2f3290922b","_uuid":"dad3d3a2-0fd4-4873-a2fc-004121d711fd","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:05.727494Z","iopub.status.busy":"2023-07-06T13:54:05.726482Z","iopub.status.idle":"2023-07-06T13:54:05.920642Z","shell.execute_reply":"2023-07-06T13:54:05.919157Z","shell.execute_reply.started":"2023-07-06T13:54:05.727459Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: you can use isna() and sum().\n","\n","train_df. # type your solution"]},{"cell_type":"markdown","metadata":{"_cell_guid":"3f859f67-2f07-4691-adad-6a71fc703695","_uuid":"1177e146-075e-44d1-b5ed-89302c2b6943","trusted":true},"source":["**Task 5** \n","\n","In order to make following tasks simpler, we will change \"satisfaction\" column from string to integer."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76c4da58-3bd1-4868-b866-ac7ffb8ac339","_uuid":"f3606895-e271-4807-b0e9-345e09a3ee61","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:05.923179Z","iopub.status.busy":"2023-07-06T13:54:05.922722Z","iopub.status.idle":"2023-07-06T13:54:05.951299Z","shell.execute_reply":"2023-07-06T13:54:05.950224Z","shell.execute_reply.started":"2023-07-06T13:54:05.923144Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: Use the .map() function and change 'neutral or dissatisfied' -> 0 and 'satisfied' -> 1\n","print(\"Values before convertion: %s\" % list(train_df['satisfaction'][:5]))\n","\n","train_df['satisfaction'] = train_df['satisfaction']. # Type your solution\n","\n","print(\"Values after convertion: %s\" % list(train_df['satisfaction'][:5]))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a81dac36-4d4f-4097-9929-c1d6ec68e52c","_uuid":"89294fc2-9dbc-4246-9b04-4655d4dfb9fb","trusted":true},"source":["**Task 6**\n","\n","Let's visualize some feature distribution! Such visualizations can give us an idea of which features are most relevant. Show \"Gender\" distribution in terms of the \"satisfaction\"."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56cda18e-1f1b-40f4-a438-b113fce9a527","_uuid":"78662df0-5f7d-453a-9178-fa182fc7bb2f","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:05.953138Z","iopub.status.busy":"2023-07-06T13:54:05.952746Z","iopub.status.idle":"2023-07-06T13:54:07.635412Z","shell.execute_reply":"2023-07-06T13:54:07.633888Z","shell.execute_reply.started":"2023-07-06T13:54:05.953105Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: you can use seaborn barplot with variables \"Gender\" and \"satisfaction\". \n","\n","sns.barplot(, )  # Type your solution"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e8acf927-4204-4074-a070-32ba5377b870","_uuid":"85dcf9b2-836b-4dd9-9a85-30e75980ba90","trusted":true},"source":["**Task 7** \n","\n","Following with our visualization approach, use a barplot once again to take a look at the passenger class distribution.\n","\n","**Q1**: Explain what this plot is reporting. Is there any class that have higher chances to be satisfied? And they say money can't buy everything..."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"faaf504f-0da7-4280-a84f-dff759faa3a1","_uuid":"155a4776-b00e-4f3c-8a9e-06411354a52b","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:07.637641Z","iopub.status.busy":"2023-07-06T13:54:07.637139Z","iopub.status.idle":"2023-07-06T13:54:09.870487Z","shell.execute_reply":"2023-07-06T13:54:09.869141Z","shell.execute_reply.started":"2023-07-06T13:54:07.637608Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: use seaborn countplot with the right axis - 0 or 1?\n","\n","f,ax=plt.subplots(1,2,figsize=(18,8))\n","train_df['Class'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\n","ax[0].set_title('Number Of Customers By Class')\n","ax[0].set_ylabel('Count')\n","\n","f = sns.barplot(, ) # Type your solution here\n","\n","ax[1].set_title('different sample sizes')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"d77eb65a-7198-4800-b51d-4541d176cbec","_uuid":"a33c6b0f-275d-4983-ae3a-908628f4e69d","trusted":true},"source":["**Task 8** \n","\n","Another important part of an EDA with tabular data is to check the correlation between features. If two features are correlated, an increase (or decrease) might lead to an increase (or decrease) in the other one. This means the information contained by both features is similar and there is little to no variance in information. This is known as multicolinearity. In essence, one of the features could be deemed as \"redundant\". A way to deal with it could be to drop one of the redundant features as it will reduce training time, to mention one of the few advantages. Another option could be to reduce the feature space by means of an autoencoder (which you'll hear about in the theoretical lectures) or reduction techniques such as principal component analysis (PCA). In this case, we will only plot a correlation heatmap for educational purposes.\n","\n","**Q2**: Which features display the largest positive and negative correlations regarding the variable \"satisfaction\"? Why not all features are included? Report the plot."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"578cb39f-4b16-47e1-b44d-13f7805fb9f1","_uuid":"d5a09d9f-1a59-4515-b272-1e508c04aab9","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:09.873600Z","iopub.status.busy":"2023-07-06T13:54:09.873001Z","iopub.status.idle":"2023-07-06T13:54:11.560642Z","shell.execute_reply":"2023-07-06T13:54:11.559150Z","shell.execute_reply.started":"2023-07-06T13:54:09.873554Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: use seaborn for plotting the heatmap. Correlation matrix from our data can be obtanied with the .corr() method.\n","\n","sns.heatmap(, annot=True, cmap='RdYlGn',linewidths=0.2, fmt='.2f') # Type your solution here\n","fig=plt.gcf()\n","fig.set_size_inches(14,10)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e088e6e3-a6c3-4195-9ee7-dbeeca958bf1","_uuid":"b5b4fcc6-7f22-4625-b5c3-c9292c646b44","trusted":true},"source":["### Data Preprocessing\n","\n","Thanks to EDA, we were able to find two main issues (we could have done a more exhaustive process to find other anomalies):\n","* Our data contains some missing values\n","* There are columns that are irrelevant for our project.\n","\n","Now, it is time to deal with them! \n","\n","Remember that, in this preprocessing step, it is a common practice to apply some feature normalization/standardization. After doing this, we will have our data prepared for training."]},{"cell_type":"markdown","metadata":{"_cell_guid":"f92f5748-3645-4e71-8b53-fc8e9e25e684","_uuid":"d32b8615-cbf5-43fd-b7c2-270c08b6fd09","trusted":true},"source":["**Task 9**\n","\n","We will drop the irrelevant variables first, such as \"id\", and then ALL the rows with empty values (NA). Define a variable *train_y* containing the outcome (\"satisfaction\"). In addition, define a variable *train_x* removing the \"satisfaction\" column, since it has been defined as our dependent variable."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a7d58c7-0898-409e-a46a-01945a7ba9b3","_uuid":"064c969d-c267-449a-a99c-85d883d1c8ee","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:11.566533Z","iopub.status.busy":"2023-07-06T13:54:11.565587Z","iopub.status.idle":"2023-07-06T13:54:11.773124Z","shell.execute_reply":"2023-07-06T13:54:11.771949Z","shell.execute_reply.started":"2023-07-06T13:54:11.566497Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: use dropna()\n","\n","train_df = train_df # Type your solution here (drop)\n","train_df = train_df # type your solution here (dropna)\n","\n","train_y =  # type your solution here\n","train_x =  # type your solution here"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a4fe865e-02fe-412b-b835-bcdfdaa97ed9","_uuid":"76ada2da-c632-45ba-a3b6-c7078d2a85a4","trusted":true},"source":["**Task 10** \n","\n","As you might have noticed, the dataset includes several non-numerical variables (categorical). In order to process them, we will need to transform them into \"dummy variables\" (we already did something similar with \"satisfaction\"). In this case, convert the non-numerical variables to dummy with get_dummies. Another suitable option can be one-hot-encoding from sklearn. Feel free to experiment with it and to find out more about the differences betweem them!."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c07235f-4169-4ab1-8def-d4ea39a4aadb","_uuid":"8bafe979-533c-460c-8bd0-7dba50a455f8","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:11.776006Z","iopub.status.busy":"2023-07-06T13:54:11.775491Z","iopub.status.idle":"2023-07-06T13:54:11.884527Z","shell.execute_reply":"2023-07-06T13:54:11.883430Z","shell.execute_reply.started":"2023-07-06T13:54:11.775962Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: use pandas get_dummies()\n","\n","train_x =  # Type your solution here\n","train_x.head()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"554c4464-1076-4ec4-954e-508b0214bf65","_uuid":"b93087ed-4d94-484a-83cf-8b2945eea8f1","trusted":true},"source":["**Task 11** \n","\n","Standardize the dataset. First, create the scaler object by using the StandardScaler() and, then, transform the data."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75af809a-0337-4367-a2e1-c0197b0d2213","_uuid":"abdf4600-5190-4272-9f4f-26147447b3be","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:11.886510Z","iopub.status.busy":"2023-07-06T13:54:11.885859Z","iopub.status.idle":"2023-07-06T13:54:11.978908Z","shell.execute_reply":"2023-07-06T13:54:11.977730Z","shell.execute_reply.started":"2023-07-06T13:54:11.886471Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: Use .fit_transform() method to first fit to data and then transform it.\n","\n","scaler =  # Type your solution here\n","\n","train_x_scaled =  # Type your solution here\n","\n","train_x_scaled = pd.DataFrame(train_x_scaled, columns=train_x.columns) # For visualization purposes, we convert the data to DataFrame\n","train_x_scaled.head()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"4ff7a692-8217-455e-88b5-0b9eade0e948","_uuid":"3ea80208-c390-4502-9d9f-1d48e4afd9fa","trusted":true},"source":["**Task 12** \n","\n","The final step is split the data in train and test subsets to get an unbiased evaluation of our model. In essence, we first make use of the training set to \"teach\" the model and make it learn patterns from the data. Then, the test set will be kept apart and not used until the model has been fully trained and we want to see the model performane on \"unseen data\". Think about it this way: You have your own business and plan to deploy a model in the real world so, how do you evaluate how good is the model when it comes to \"new data\" that you did not have when you trained the model? That is where the test set comes in handy!\n","\n","This step can be quite tedious as you need to ensure your test set is representative enough of the actual problem to solve. It is important to consider the distribution of your data, the proportion of labels, the split percentage between train and test...\n","\n","Luckily for us, our dataset is already split! So, once we've encoded our non-numerical variables and scaled the features for our training data, we just need to repeat the same procedure on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73cb169a-acce-4d74-af65-ab659e636cfb","_uuid":"419b2de2-a177-4153-a197-72d2f023e102","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:11.980862Z","iopub.status.busy":"2023-07-06T13:54:11.980449Z","iopub.status.idle":"2023-07-06T13:54:12.169945Z","shell.execute_reply":"2023-07-06T13:54:12.168578Z","shell.execute_reply.started":"2023-07-06T13:54:11.980825Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Hint: For scaling the test set, we'll' use the previous scaler to use the same convertion used for the training set. Use .transform() method\n","\n","# Read data\n","test_df = pd.read_csv(, index_col=0) # type your solution here\n","\n","# Convert \"satisfaction\" column from string to integer\n","test_df['satisfaction'] =  # type your solution here\n","\n","# Drop irrelevant variables and NA values\n","test_df =  # type your solution here (drop)\n","test_df =  # type your solution here (dropna)\n","\n","# Define variables test_x and test_y\n","test_y =  # type your solution here\n","test_x =  # type your solution here\n","\n","# Convert non-numerical variables (categorical) into dummies variables.\n","test_x =  # type your solution here\n","\n","# Standardize the test set\n","test_x_scaled = # Type your solution here\n","\n","test_x_scaled = pd.DataFrame(test_x_scaled, columns=test_x.columns) # For visualization purposes, we convert the data to DataFrame\n","\n","print(\"Shape of the training set:\", train_x_scaled.shape)\n","print(\"Shape of the training labels:\", train_y.shape)\n","print(\"Shape of the testing set:\", test_x_scaled.shape)\n","print(\"Shape of the testing labels:\", test_y.shape)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"c97a3d17-c699-4e72-9d74-981ed602b8a7","_uuid":"258af78d-2710-4ef1-b635-89f06631a196","trusted":true},"source":["### MLP Implementation\n","\n","Time to create our classifier! We will design a very simple MLP classifier consisting of 3 layers. Take into consideration that the design of a classifier needs time and a proper practice would be to either test the configurations or simply select an existing architecture such as ResNet or EfficientNet."]},{"cell_type":"markdown","metadata":{"_cell_guid":"9de97b53-2fac-4295-864e-6889a9169ab1","_uuid":"133601a0-f911-48ce-bd9d-0c7bdb5f1d84","trusted":true},"source":["**Task 13**\n","\n","Design a classifier with 3 hidden layers of 25, 50 and 100 neurons, respectively, with ReLU as activation function. Remember to indicate the right input dimension in the first layer. What about the last layer? How many classes are we predicting? What activation function should we choose considering that we are aiming for a classification problem?"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"534c0f71-f7a5-4247-ac7a-2f84fbec3c82","_uuid":"bca88c32-1946-4f8b-8775-efff15180e46","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:12.172729Z","iopub.status.busy":"2023-07-06T13:54:12.171915Z","iopub.status.idle":"2023-07-06T13:54:12.183047Z","shell.execute_reply":"2023-07-06T13:54:12.181862Z","shell.execute_reply.started":"2023-07-06T13:54:12.172685Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Before creating the model.. transform your training labels to categorical with to_categorical() function.\n","train_y_cat = # type your solution\n","\n","def mlp_model():\n","    \n","    mlp = Sequential()\n","    mlp.add() # Type your solution\n","    mlp.add() # Type your solution*\n","    mlp.add() # Type your solution\n","    mlp.add() # Type your solution\n","    \n","    mlp.summary()\n","    \n","    return mlp"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e323b680-501c-4501-a35a-120eca96c7ca","_uuid":"8b23b171-85ff-4b9a-8ec6-888a9e1f34ce","trusted":true},"source":["**Task 14** \n","\n","Once we have defined our model, we will proceed to train it! The first thing we need to do is to define the optimizer. There are several options available (feel free to explore them: https://keras.io/api/optimizers/) but for now, we'll make use of Stochastic Gradient Descent (SGD). We will set a fixed learning rate of 0.001. Following, we compile the model and choose our loss function (which loss function is the most suitable for this task?) and metrics. Finally, we fit the model. Here, one needs to decide the batch size, the number of epochs and callbacks (if any). We'll use a batch size of 128 and 200 epochs (it may take around 4-5 min). In addition, we will use an additional validation set (validation_split = 0.2) which will allow us to closely monitorize the values of our loss and accuracy while training. This gives us an initial idea of how the model is performing in an independent set. \n","\n","**Q3**: In this case, we've used accuracy as a metric to evaluate our ML algorithm. Do you think it is a suitable one? Why? Is there any other metric we could use better than our choice?"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16b47690-4a9b-4854-923c-43fcb99e15cc","_kg_hide-output":false,"_uuid":"e1b88dd9-1bf5-4e75-9ebb-04df2af8cfdb","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T13:54:12.185907Z","iopub.status.busy":"2023-07-06T13:54:12.184978Z","iopub.status.idle":"2023-07-06T14:00:43.006431Z","shell.execute_reply":"2023-07-06T14:00:43.004524Z","shell.execute_reply.started":"2023-07-06T13:54:12.185864Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["mlp = mlp_model()\n","optimizer =  # Type your solution\n","mlp.compile(optimizer=optimizer, loss=\"\", metrics=[\"accuracy\"]) # Type your solution. Compile the model!\n","history = mlp.fit() # Type your solution"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8f699743-f4f3-4e47-bc7c-bd68bb02649a","_uuid":"3a87524c-ddf4-4041-a922-c2a9e62067d1","trusted":true},"source":["**Task 15** \n","\n","Plot accuracy and loss curves.\n","\n","**Q4**: After seeing the curves - do you think it is necessary to train for 200 epochs? Is there any way to automatically \"control\" for how long we train without the need to train for the initially specified number of epochs?"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2368f8ba-1cc3-4be3-898f-65156051768c","_uuid":"f19835d1-3b8e-48f3-9726-9e8c978bb803","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T14:00:43.009100Z","iopub.status.busy":"2023-07-06T14:00:43.008571Z","iopub.status.idle":"2023-07-06T14:00:43.795779Z","shell.execute_reply":"2023-07-06T14:00:43.794531Z","shell.execute_reply.started":"2023-07-06T14:00:43.009059Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["f,ax=plt.subplots(1,2,figsize=(18,8))\n","\n","ax[0].plot() # Type your solution\n","ax[0].plot() # Type your solution\n","ax[0].set_title('model accuracy')\n","ax[0].set_ylabel('accuracy')\n","ax[0].set_xlabel('epoch')\n","ax[0].legend(['train', 'val'], loc='upper left')\n","\n","ax[1].plot() # Type your solution\n","ax[1].plot() # Type your solution\n","ax[1].set_title('model loss')\n","ax[1].set_ylabel('loss')\n","ax[1].set_xlabel('epoch')\n","ax[1].legend(['train', 'val'], loc='upper left')"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a3a467a1-963f-4696-bd60-38cf3172fc05","_uuid":"89000e54-4e03-4ce8-9732-dc493dfb7ef6","execution":{"iopub.status.busy":"2023-07-05T15:38:30.974585Z","iopub.status.idle":"2023-07-05T15:38:30.975390Z","shell.execute_reply":"2023-07-05T15:38:30.975142Z","shell.execute_reply.started":"2023-07-05T15:38:30.975114Z"},"trusted":true},"source":["### Predictions on test set\n","\n","Once the model has been trained, we can make predictions on our test set."]},{"cell_type":"markdown","metadata":{"_cell_guid":"0ab6b359-0070-4cee-bee1-2f8e1349b0a2","_uuid":"cc06e8d6-4de9-4358-97de-4a6e2b31eecb","trusted":true},"source":["**Task 16**\n","\n","Predict the class probability for the test set data. Convert those probabilities into a class prediction by taking the argmax() of them.\n","Compute the accuracy for your test set and plot a confusion matrix.\n","\n","**Q5**: What is reporting this confusion matrix?"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb9bfc24-bcb5-4107-aacc-6cc75d0411c8","_uuid":"d0b2b35d-dc6f-4312-bf86-88e4c7d1b7e0","collapsed":false,"execution":{"iopub.execute_input":"2023-07-06T14:00:43.798658Z","iopub.status.busy":"2023-07-06T14:00:43.797892Z","iopub.status.idle":"2023-07-06T14:00:45.502466Z","shell.execute_reply":"2023-07-06T14:00:45.500998Z","shell.execute_reply.started":"2023-07-06T14:00:43.798605Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["predictions = mlp.predict() # Type your solution\n","\n","# Hint - obtain argmax of predictions\n","pred_argmax = np.argmax() # Type your solution\n","\n","# Obtain accuracy with argmax\n","acc_test = accuracy_score() # Type your solution\n","# Obtain confusion matrix with argmax\n","conf_mat = confusion_matrix() # Type your solution\n","\n","print(\"Accuracy test:\", acc_test)\n","print(\"Confusion matrix:\", conf_mat)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"e27a9fc8-2fb9-49c9-ab7e-f956cde038f8","_uuid":"c9eea0cc-9645-4440-bdcc-db8e640e8087","jupyter":{"outputs_hidden":false},"trusted":true},"source":["### QUESTIONS\n","\n","**Q1** (Task 7): \n","\n","**Q2** (Task 8): \n","\n","**Q3** (Task 14): \n","\n","**Q4** (Task 15): \n","\n","**Q5** (Task 16): "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9bfc7849-0bf5-4765-a926-802246e2f150","_uuid":"653641a6-679c-4d6a-a918-2e42d40195d7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
