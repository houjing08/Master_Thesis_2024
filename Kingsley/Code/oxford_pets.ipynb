{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of custom_classes_defs.hed failed: Traceback (most recent call last):\n",
      "  File \"/Users/bawfeh78/miniconda3/envs/tf2_py310/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/bawfeh78/miniconda3/envs/tf2_py310/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/bawfeh78/miniconda3/envs/tf2_py310/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/bawfeh78/Documents/UIS/Masters/Code/custom_classes_defs/hed.py\", line 1, in <module>\n",
      "    from setup import *\n",
      "ModuleNotFoundError: No module named 'setup'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last\" # all | last | last_expr | none "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dir():\n",
    "    if not name.startswith('_'):\n",
    "        del globals()[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Import required packaages ==============\n",
    "import re, time, os\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Import all custom variables and modules\n",
    "from custom_classes_defs.preprocessing import *\n",
    "# from custom_classes_defs.Unet_like import *   \n",
    "# from custom_classes_defs.fnet import *\n",
    "from custom_classes_defs.hed import *\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "RND_STATE = 247\n",
    "BATCH_SIZE = 32\n",
    "keras.utils.set_random_seed(RND_STATE)\n",
    "\n",
    "INTERACTIVE_SESSION = True\n",
    "\n",
    "# from tensorflow.data import Dataset as tf_data\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 7390\n",
      "training data (size = 672)\n",
      "validation data (size = 96)\n",
      "test data (size = 6656)\n",
      "Data images tensor: TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name=None)\n",
      "Data labels tensor: TensorSpec(shape=(None, 160, 160, 1), dtype=tf.uint8, name=None)\n"
     ]
    }
   ],
   "source": [
    "pets = Oxford_Pets(\n",
    "        input_dir = \"./data/images/\",\n",
    "        target_dir = \"./data/annotations/trimaps/\",\n",
    "        img_size = (160, 160),\n",
    "        batch_size = BATCH_SIZE\n",
    "    )\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = \\\n",
    "        pets.split_data(seed=RND_STATE)\n",
    "\n",
    "#for input_path, target_path in zip(pets.input_img_paths[:10], pets.target_img_paths[:10]):\n",
    "#    print(input_path, \"|\", target_path)\n",
    "print(f\"training data (size = {len(train_dataset)*BATCH_SIZE})\")\n",
    "print(f\"validation data (size = {len(valid_dataset)*BATCH_SIZE})\")\n",
    "print(f\"test data (size = {len(test_dataset)*BATCH_SIZE})\")\n",
    "print(\"Data images tensor:\",train_dataset.element_spec[0])\n",
    "print(\"Data labels tensor:\",train_dataset.element_spec[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[334], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display input image #idx\u001b[39;00m\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m19\u001b[39m\n\u001b[0;32m----> 3\u001b[0m display(\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_img_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Display auto-contrast version of corresponding target (per-pixel categories)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m ImageOps\u001b[38;5;241m.\u001b[39mautocontrast(load_img(pets\u001b[38;5;241m.\u001b[39mtarget_img_paths[idx]))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2_py310/lib/python3.10/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2_py310/lib/python3.10/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2_py310/lib/python3.10/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2_py310/lib/python3.10/site-packages/IPython/core/display.py:354\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlopen\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Display input image #idx\n",
    "idx = 19\n",
    "display(Image(filename=pets.input_img_paths[idx]))\n",
    "\n",
    "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "img = ImageOps.autocontrast(load_img(pets.target_img_paths[idx]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model configurations\n",
    "conf = model_config(\n",
    "    epochs=30,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    scaling=1,\n",
    "    verbose=1,\n",
    "    save_path='./data',\n",
    "    augmentation=False,\n",
    "    img_shape=pets.img_size,\n",
    "    target_size=pets.img_size,\n",
    "    channels_dim=(3,3)\n",
    ")\n",
    "\n",
    "\n",
    "es_callbacks = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    #keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True),\n",
    "    es_callbacks\n",
    "]\n",
    "\n",
    "conf.set( validation_data=valid_dataset,  callbacks=callbacks )\n",
    "conf.set(\n",
    "    'compile',\n",
    "    optimizer=keras.optimizers.Adam(1e-4), \n",
    "    loss=\"sparse_categorical_crossentropy\"\n",
    ")\n",
    "conf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive step:\n",
    "if INTERACTIVE_SESSION:\n",
    "    train = input(\"New train session? (y/n): \")\n",
    "    if train[0].lower()=='y':\n",
    "        conf.new_training_session = True\n",
    "    else:\n",
    "        conf.new_training_session = False\n",
    "    interact = input(\"Are you sure, you want to run this session interactively? (y/n): \")\n",
    "    if interact[0].lower()!='y':\n",
    "        INTERACTIVE_SESSION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n{}\\n\\t{}\\n{}\".format('='*55,f'Build model', '-'*55))\n",
    "# m_obj = UNET2D(panel_sizes=[32,64,128,256], model_arch=conf.model_arch)\n",
    "m_obj = HED2D(model_arch=conf.model_arch)\n",
    "model = m_obj.build_model()\n",
    "model.summary()\n",
    "# plot_model(model, 'm_obj.png',show_shapes=True)\n",
    "print(f\"Total trainable wieghts: {model.count_params():,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if model.name == 'hed':\n",
    "#    # Enforce transfer learning\n",
    "#    for ly in model.layers:\n",
    "#        if ly.trainable and re.search('block[1-5]_conv', ly.name):\n",
    "#            ly.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "# We use the \"sparse\" version of categorical_crossentropy\n",
    "# because our target data is integers.\n",
    "\n",
    "print(\"\\n\\n{}\\n\\t{}\\n{}\".format('='*55,f'Train {m_obj.Name} model', '-'*55))\n",
    "\n",
    "conf.execute_training(\n",
    "    model, \n",
    "    data=train_dataset, \n",
    "    saveas=m_obj.Name+'_pets', \n",
    "    plot_history=INTERACTIVE_SESSION\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all images in the validation set\n",
    "\n",
    "val_preds = model.predict(valid_dataset)\n",
    "\n",
    "if INTERACTIVE_SESSION:\n",
    "    pets.display_sample_image(val_preds, image_id=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
