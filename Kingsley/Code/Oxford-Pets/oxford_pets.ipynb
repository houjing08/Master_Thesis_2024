{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last\" # all | last | last_expr | none "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in dir():\n",
    "#     if not name.startswith('_'):\n",
    "#         del globals()[name]\n",
    "\n",
    "!tar -xf ./data/images.tar.gz\n",
    "!tar -xf ./data/annotations.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Import required packaages ==============\n",
    "# Import all custom variables and modules\n",
    "from custom_classes_defs.preprocessing import *  \n",
    "# from custom_classes_defs.unet0 import *\n",
    "# from custom_classes_defs.Unet_like import *   \n",
    "# from custom_classes_defs.unet import *\n",
    "from custom_classes_defs.fnet0 import *\n",
    "# from custom_classes_defs.fnet_like import *\n",
    "# from custom_classes_defs.fnet import *\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "RND_STATE = 247\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "keras.utils.set_random_seed(RND_STATE)\n",
    "\n",
    "INTERACTIVE_SESSION = True\n",
    "\n",
    "# import keras_tuner as kt\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tensorflow/keras versions\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "print(f\"keras version: {keras.__version__}\")\n",
    "\n",
    "# Verify CPU/GPU availability\n",
    "print(tf.config.list_physical_devices())\n",
    "NUM_GPU = len(tf.config.list_physical_devices('GPU'))\n",
    "print(f\"Number of GPUs assigned for computation: {NUM_GPU}\")\n",
    "\n",
    "if NUM_GPU:\n",
    "    # print GPU info\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = Oxford_Pets(\n",
    "        input_dir = \"./data/images/\",\n",
    "        target_dir = \"./data/annotations/trimaps/\",\n",
    "        img_size = (160, 160),\n",
    "        batch_size = BATCH_SIZE\n",
    "    )\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = \\\n",
    "        pets.split_data(train_ratio=0.5, val_ratio=0.2, seed=RND_STATE)\n",
    "\n",
    "\n",
    "print(f\"training data (size = {pets.train_size})\")\n",
    "print(f\"validation data (size = {pets.validation_size})\")\n",
    "print(f\"test data (size = {pets.test_size})\")\n",
    "print(\"Data images tensor:\",train_dataset.element_spec[0])\n",
    "print(\"Data labels tensor:\",train_dataset.element_spec[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape = (pets.train_size,) + tuple(train_dataset.element_spec[-1].shape[1:])\n",
    "cl_weights = np.ones(train_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model configurations\n",
    "conf = model_config(\n",
    "    epochs=EPOCHS,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    save_path='./oxford-pets/fnet00',\n",
    "    img_shape=pets.img_size,\n",
    "    target_size=pets.img_size,\n",
    "    train_size=pets.train_size,\n",
    "    test_size=pets.test_size,\n",
    "    validation_size=pets.validation_size,\n",
    "    channels_dim=(3,3),\n",
    "    pos_label=pets.pos_label,\n",
    "    new_training_session=True,\n",
    "    multiple_gpu_device=(NUM_GPU>1),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "callbacks=conf.callbacks()\n",
    "conf.set( validation_data=valid_dataset,  callbacks=callbacks)\n",
    "conf.set(\n",
    "    'compile',\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(1e-4), \n",
    "    metrics= ['accuracy'],\n",
    ")\n",
    "\n",
    "# conf.double_check(INTERACTIVE_SESSION)\n",
    "conf.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SINGLE-HOST, MULTI-DEVICE SYNCHRONOUS TRAINING\n",
    "## Fran√ßois Chollet. Deep Learning with Python, Second Edition (Kindle Location 12675). Manning Publications Co.. \n",
    "print(\"\\n\\n{}\\n\\t{}\\n{}\".format('='*55,f'Build model', '-'*55))\n",
    "\n",
    "if conf.multiple_gpu_device:\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "    with strategy.scope():\n",
    "        # m_obj = UNET2D(panel_sizes=[32,64,128,256], model_arch=conf.model_arch)\n",
    "        m_obj = FNET2D(panel_sizes=[32,64,128,256], model_arch=conf.model_arch, add_residual=False)\n",
    "        model = m_obj.build_model()\n",
    "        model.compile(**conf.compile_args)\n",
    "\n",
    "else:\n",
    "\n",
    "    # m_obj = UNET2D(panel_sizes=[32,64,128,256], model_arch=conf.model_arch)\n",
    "    m_obj = FNET2D(panel_sizes=[32,64,128,256], model_arch=conf.model_arch, add_residual=False)\n",
    "    model = m_obj.build_model()\n",
    "    model.compile(**conf.compile_args)\n",
    "\n",
    "# model.summary()\n",
    "plot_model(model, 'm_obj.png',show_shapes=True)\n",
    "num_trainable_weights = sum([np.prod(w.shape) for w in model.trainable_weights])\n",
    "print(f\"Total number of parameters: {model.count_params():,}\")\n",
    "print(f\"Total trainable wieghts: {num_trainable_weights:,}\")\n",
    "print(f\"Total non-trainable wieghts: {model.count_params()-num_trainable_weights:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n{}\\n\\t{}\\n{}\".format('='*55,f'Train {m_obj.Name} model', '-'*55))\n",
    "\n",
    "model, train_history = \\\n",
    "    conf.execute_training(\n",
    "        model, \n",
    "        data=train_dataset, \n",
    "        plot_history=INTERACTIVE_SESSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_convergence(train_history.history, ['accuracy','val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_convergence(train_history.history, 'lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all images in the validation set\n",
    "y_preds = model.predict(test_dataset, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERACTIVE_SESSION:\n",
    "    pets.display_sample_image(y_preds, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\\n{}\\n\\t{}\\n{}\".format('='*55,f'Evaluate {m_obj.Name} model', '-'*55))\n",
    "\n",
    "# model.evaluate(x=test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scikit-learn\n",
    "scores = m_obj.evaluate_sklearn(test_dataset, y_preds,report=True)\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
